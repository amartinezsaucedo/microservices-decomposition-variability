{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PROJECT_PATH = \"\" # one of [cargo, jpetstore]\n",
    "PROJECT_NAME = \"\" # one of [cargo, jpetstore]\n",
    "GROQ_API_KEY = \"\" # create at https://console.groq.com/keys"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.groq import Groq\n",
    "from semantic_router import Route\n",
    "from semantic_router.encoders import HuggingFaceEncoder\n",
    "from semantic_router.layer import RouteLayer\n",
    "from IPython.display import Markdown\n",
    "from ema_workbench import load_results\n",
    "from ema_workbench.analysis import feature_scoring\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from typing import Union\n",
    "from SALib.analyze import sobol\n",
    "import random\n",
    "import collections\n",
    "from groq import Groq as GroqV2\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.core.tools.query_engine import QueryEngineTool\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors.llm_selectors import (\n",
    "    LLMSingleSelector\n",
    ")\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import os\n",
    "from IPython.display import display\n",
    "random.seed(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('httpcore').setLevel(logging.WARNING)\n",
    "logging.getLogger('llama_index').setLevel(logging.WARNING)\n",
    "logging.getLogger('EMA').setLevel(logging.WARNING)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "COMMON_COLUMNS = ['algorithm', 'scenario', 'policy', 'model']\n",
    "\n",
    "PARAMETER_MAPPING = {\n",
    "    'microservice_threshold': 'mthreshold',\n",
    "}\n",
    "\n",
    "def get_parameters(experiments: pd.DataFrame) -> list[str]:\n",
    "    parameters = list(set(COMMON_COLUMNS) ^ set(experiments.columns.values))\n",
    "    parameters.sort()\n",
    "    return parameters\n",
    "\n",
    "def get_normalized_parameter(parameter: str) -> str:\n",
    "    new_parameter = PARAMETER_MAPPING.get(parameter)\n",
    "    if new_parameter is not None:\n",
    "        return new_parameter\n",
    "    return parameter\n",
    "\n",
    "\n",
    "def reorder_index(index):\n",
    "    parts = index.split('_')\n",
    "    pairs = list(zip(parts[0::2], parts[1::2]))\n",
    "    pairs.sort(key=lambda x: x[0])\n",
    "    return '_'.join([f\"{parameter}_{float(value):.9f}\" for parameter, value in pairs])\n",
    "\n",
    "def rename_dict_keys(dictionary: dict) -> dict:\n",
    "    new_dictionary = {}\n",
    "    for key, value in dictionary.items():\n",
    "        new_dictionary[reorder_index(key)] = value\n",
    "    return new_dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DecompositionSpace:\n",
    "    def __init__(self, project_path, project_name):\n",
    "        self.outcomes = pd.DataFrame()\n",
    "        self.experiments = pd.DataFrame()\n",
    "        self.all = pd.DataFrame()\n",
    "        self.partitions = dict()\n",
    "        self.uncertainties_problem = dict()\n",
    "        self.sobol = dict()\n",
    "        self.partitions_distance = pd.DataFrame()\n",
    "        self.feature_scoring = dict()\n",
    "        self.partition_labels = dict()\n",
    "        self.embeddings_2d_partitions = dict()\n",
    "        self.partitions_df = pd.DataFrame()\n",
    "        self.sobol_df = pd.DataFrame()\n",
    "        for _, algorithms, _ in os.walk(project_path):\n",
    "            if len(algorithms) > 0:\n",
    "                for algorithm in algorithms:\n",
    "                    project_full_path = f\"{project_path}/{algorithm}/{project_name}\"\n",
    "                    model_filename = f\"{project_full_path}_128scenarios_nopolicies_sobol\"\n",
    "                    experiments_df, outcomes = load_results(f\"{model_filename}.tar.gz\")\n",
    "                    experiments_df['algorithm'] = algorithm\n",
    "                    outcomes_df = pd.DataFrame(outcomes)\n",
    "                    self.outcomes = pd.concat([self.outcomes, outcomes_df])\n",
    "                    self.experiments = pd.concat([self.experiments, experiments_df])\n",
    "                    all_df = pd.concat([experiments_df, outcomes_df], axis=1)\n",
    "                    parameters = get_parameters(experiments_df)\n",
    "                    all_df.index = all_df.apply(lambda row: '_'.join([f\"{get_normalized_parameter(parameter)}_{row[parameter]:.9f}\" for parameter in parameters]), axis=1)\n",
    "                    all_df = all_df[~all_df.index.duplicated(keep='first')]\n",
    "                    all_df['decomposition_id'] = all_df.index\n",
    "                    self.all = pd.concat([self.all, all_df])\n",
    "                    with open(f\"{model_filename}_model.pkl\", 'rb') as file:\n",
    "                        self.uncertainties_problem[algorithm] = pickle.load(file)\n",
    "                    with open(f\"{model_filename}_partitions.pkl\", 'rb') as file:\n",
    "                        partitions = pickle.load(file)\n",
    "                        partitions = rename_dict_keys(partitions)\n",
    "                        self.partitions[algorithm] = partitions\n",
    "                    partitions_distance = 1 - pd.read_csv(f\"{project_full_path}_omega_scores.csv\", index_col=0)\n",
    "                    partitions_distance.index = [reorder_index(idx) for idx in partitions_distance.index]\n",
    "                    partitions_distance.columns = list(map(lambda c: reorder_index(c), partitions_distance.columns))\n",
    "                    result = pd.DataFrame(np.nan, index=pd.Index(self.partitions_distance.index.tolist() + partitions_distance.index.tolist(), name='index'),\n",
    "                                          columns = pd.Index(self.partitions_distance.columns.tolist() + partitions_distance.columns.tolist(), name='columns'))\n",
    "                    result.loc[self.partitions_distance.index, self.partitions_distance.columns] = self.partitions_distance\n",
    "                    result.loc[partitions_distance.index, partitions_distance.columns] = partitions_distance\n",
    "                    self.partitions_distance = result\n",
    "                    with open(f\"{project_full_path}_stable_solutions.pkl\", 'rb') as f:\n",
    "                        stable_solutions = pickle.load(f)\n",
    "                        other_labels = stable_solutions.keys()\n",
    "                        self.all['stability'] = 0\n",
    "                        for index, value in stable_solutions.items():\n",
    "                            self.all.loc[reorder_index(index), 'stability'] = value\n",
    "                    mds = MDS(dissimilarity='precomputed', random_state=0)\n",
    "                    self.embeddings_2d_partitions[algorithm] = mds.fit_transform(partitions_distance)\n",
    "                    partition_labels_2d, _, silhouette = self._run_agglomerative(self.embeddings_2d_partitions[algorithm], k=5, threshold=None, normalize=True, n_pca=2)\n",
    "                    self.partition_labels[algorithm] = partition_labels_2d\n",
    "                    self.feature_scoring[algorithm] = feature_scoring.get_feature_scores_all(experiments_df, outcomes)\n",
    "                    self.sobol[algorithm] = {}\n",
    "                    self.sobol[algorithm][\"n_partitions\"] = sobol.analyze(self.uncertainties_problem[algorithm], outcomes['n_partitions'], calc_second_order=True)\n",
    "                    self.sobol[algorithm][\"modularity\"] = sobol.analyze(self.uncertainties_problem[algorithm], outcomes['modularity'], calc_second_order=True)\n",
    "                    self.sobol[algorithm][\"ned\"] = sobol.analyze(self.uncertainties_problem[algorithm], outcomes['ned'], calc_second_order=True)\n",
    "                    self.sobol[algorithm][\"density\"] = sobol.analyze(self.uncertainties_problem[algorithm], outcomes['density'], calc_second_order=True)\n",
    "                    self.sobol[algorithm][\"noise_classes\"] = sobol.analyze(self.uncertainties_problem[algorithm], outcomes['noise_classes'], calc_second_order=True)\n",
    "                    self.class_color = {}\n",
    "                    self.legends = {}\n",
    "                    resolution = []\n",
    "                    partition_id = []\n",
    "                    class_name = []\n",
    "                    for res, partitions_2 in partitions.items():\n",
    "                        for part_id, classes in partitions_2.items():\n",
    "                            for cls in classes:\n",
    "                                resolution.append(res)\n",
    "                                partition_id.append(part_id)\n",
    "                                class_name.append(cls)\n",
    "                    partitions_df = pd.DataFrame({\n",
    "                        'decomposition_id': resolution,\n",
    "                        'microservice_id': partition_id,\n",
    "                        'class_name': class_name,\n",
    "                        'algorithm': algorithm\n",
    "                    })\n",
    "                    self.partitions_df = pd.concat([self.partitions_df, partitions_df])\n",
    "                    metrics = []\n",
    "                    partitions_3 = []\n",
    "                    value_1 = []\n",
    "                    value_2 = []\n",
    "                    for metric, partitions_dict in self.sobol[algorithm].items():\n",
    "                        for partition, values in partitions_dict.items():\n",
    "                            if values.ndim == 1:\n",
    "                                metrics.append(metric)\n",
    "                                partitions_3.append(partition)\n",
    "                                value_1.append(values[0])\n",
    "                                value_2.append(values[1])\n",
    "                            elif values.ndim == 2:\n",
    "                                for i in range(values.shape[0]):\n",
    "                                    metrics.append(metric)\n",
    "                                    partitions_3.append(partition)\n",
    "                                    value_1.append(values[i, 0])\n",
    "                                    value_2.append(values[i, 1])\n",
    "\n",
    "                    sobol_df = pd.DataFrame({\n",
    "                        'algorithm': algorithm,\n",
    "                        'metric': metrics,\n",
    "                        'Sobol index': partitions_3,\n",
    "                        'Variance lower': value_1,\n",
    "                        'Variance upper': value_2\n",
    "                    })\n",
    "                    self.sobol_df = pd.concat([self.sobol_df, sobol_df])\n",
    "\n",
    "    def _run_agglomerative(self, df, k, threshold=200, n_pca=None, normalize=False, archstructure=None):\n",
    "        if normalize:\n",
    "            sample = StandardScaler().fit_transform(df)\n",
    "        else:\n",
    "            sample = df.values\n",
    "\n",
    "        if n_pca is not None:\n",
    "            sample_pca = sample\n",
    "            model = AgglomerativeClustering(n_clusters=k, metric='euclidean', linkage='ward',\n",
    "                                            connectivity=archstructure, distance_threshold=threshold)\n",
    "            model.fit(sample_pca)\n",
    "        else:\n",
    "            model = AgglomerativeClustering(n_clusters=k, metric='precomputed', linkage='single',\n",
    "                                            connectivity=archstructure, distance_threshold=threshold)\n",
    "            model.fit(sample)\n",
    "        fixed_labels = np.where(model.labels_ < 0, 0, model.labels_)\n",
    "        classes = set(fixed_labels)\n",
    "        if len(classes) > 1:\n",
    "            if n_pca is not None:\n",
    "                silhouette = metrics.silhouette_score(sample_pca, fixed_labels)\n",
    "            else:\n",
    "                silhouette = metrics.silhouette_score(sample, fixed_labels)\n",
    "        else:\n",
    "            silhouette = 0.0\n",
    "        return fixed_labels, model, silhouette\n",
    "\n",
    "    def _select_medoids_from_clusters(self, algorithm):\n",
    "        partition_labels = self.partition_labels[algorithm]\n",
    "        distance_df = self.partitions_distance\n",
    "        embeddings_2d = self.embeddings_2d_partitions[algorithm]\n",
    "        classes = set(partition_labels)\n",
    "        medoids = []\n",
    "        medoid_labels = []\n",
    "        for c in classes:\n",
    "            cluster_indices = [i for i, x in enumerate(partition_labels) if x == c]\n",
    "            cluster_distances = []\n",
    "            for i in cluster_indices:\n",
    "                cluster_distances.append(np.sum(distance_df.iloc[i, cluster_indices]))\n",
    "            idx = cluster_indices[np.argmin(cluster_distances)]\n",
    "            medoid_labels.append(distance_df.columns[idx])\n",
    "            kmodel = KMedoids(n_clusters=1, method='pam').fit(embeddings_2d[cluster_indices])\n",
    "            medoids.extend(kmodel.cluster_centers_)\n",
    "        return medoids, medoid_labels\n",
    "\n",
    "    def _show_sobol(self, Si, algorithm, title='', filename=None):\n",
    "        scores_filtered = {k:Si[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "        Si_df = pd.DataFrame(scores_filtered, index=self.uncertainties_problem[algorithm]['names'])\n",
    "\n",
    "        sns.set_style('white')\n",
    "        fig, ax = plt.subplots(1)\n",
    "\n",
    "        indices = Si_df[['S1','ST']]\n",
    "        err = Si_df[['S1_conf','ST_conf']]\n",
    "\n",
    "        indices.plot.bar(yerr=err.values.T,ax=ax)\n",
    "        fig.set_size_inches(8,6)\n",
    "        fig.subplots_adjust(bottom=0.3)\n",
    "        plt.title(title)\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename)\n",
    "        return plt\n",
    "\n",
    "    def show_pairplot(self, title='', group='scenario'):\n",
    "        data = pd.DataFrame(self.outcomes)\n",
    "        policies = self.experiments['scenario']\n",
    "        data['scenario'] = policies\n",
    "        g = sns.pairplot(data, hue=group, vars=list(self.outcomes.keys()), corner=True, plot_kws={'alpha':0.25})\n",
    "        g._legend.remove()\n",
    "        g.fig.suptitle(title)\n",
    "        return plt\n",
    "\n",
    "    def get_stable_solutions(self) -> plt.plot:\n",
    "        num_plots = len(self.embeddings_2d_partitions.items())\n",
    "        cols = 2\n",
    "        rows = (num_plots + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "        axes = axes.flatten()\n",
    "        for idx, (algorithm, values) in enumerate(self.embeddings_2d_partitions.items()):\n",
    "            df = pd.DataFrame(values, columns=['x', 'y'])\n",
    "            df['stability'] = self.all.loc[self.all['algorithm'] == algorithm]['stability'].tolist()\n",
    "            df.sort_values('stability', inplace=True) # To plot it correctly\n",
    "            ax = axes[idx]\n",
    "            sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"stability\", palette=\"magma_r\", edgecolors='dimgrey',  alpha=0.7, s=35, marker='o', ax=ax)\n",
    "            ax.set(xlabel=None)\n",
    "            ax.set(ylabel=None)\n",
    "            ax.set_title(f'Stable decompositions - {algorithm}', fontsize=12)\n",
    "            plt.grid(False)\n",
    "        return plt\n",
    "\n",
    "    def _get_xy_coordinates(self, labels, algorithm):\n",
    "        distance_df = self.partitions_distance\n",
    "        embeddings_2d = self.embeddings_2d_partitions[algorithm]\n",
    "        xy_coordinates = []\n",
    "        for lb in labels:\n",
    "            idx = list(distance_df.columns).index(lb)\n",
    "            xy_coordinates.append(embeddings_2d[idx])\n",
    "        return xy_coordinates\n",
    "\n",
    "    def get_decomposition_space(self, algorithm: str, labels=None) -> plt.plot:\n",
    "        if labels is None:\n",
    "            labels = []\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        df = pd.DataFrame(self.embeddings_2d_partitions[algorithm], columns=['x', 'y'])\n",
    "        df['cluster'] = self.partition_labels[algorithm]\n",
    "        ax = sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"cluster\", palette=\"tab10\", alpha=0.3, sizes=(20, 200))\n",
    "        plt.legend([],[], frameon=False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        ax.set(xlabel=None)\n",
    "        ax.set(ylabel=None)\n",
    "        ax.set_title(f\"{algorithm}\", fontsize=12)\n",
    "        if len(labels) > 0:\n",
    "            medoids = np.array(self._get_xy_coordinates(labels, algorithm))\n",
    "            if medoids.size != 0:\n",
    "                ax.plot(medoids[:,0], medoids[:,1], 'X', markersize=9, alpha=0.7, color='black')\n",
    "                for idx, label in enumerate(labels):\n",
    "                    parts = label.split('_')\n",
    "                    formatted_label = '\\n'.join([f\"{parts[i]}: {int(float(parts[i+1]))}\" for i in range(0, len(parts), 2)])\n",
    "                    ax.annotate(formatted_label, (medoids[idx,0], medoids[idx,1]))\n",
    "        plt.grid(False)\n",
    "        return plt\n",
    "\n",
    "    def plot_2d_embeddings(self, algorithm: str, title: str = \"\") -> plt.plot:\n",
    "        embeddings_2d = self.embeddings_2d_partitions[algorithm]\n",
    "        partitions_dict = self.partitions[algorithm]\n",
    "        df = pd.DataFrame(embeddings_2d, columns=['x', 'y'])\n",
    "        df['cluster size'] = [len(partitions_dict[k]) for k in partitions_dict.keys()]\n",
    "        ax = sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"cluster size\", palette=\"tab10\", size=\"cluster size\",  alpha=0.3, legend='full', sizes=(20, 200))\n",
    "        plt.title(title)\n",
    "        ax.legend(loc='upper right',ncol=2, title=\"n_partitions\", bbox_to_anchor=(1.3, 1.05))\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        ax.set(xlabel=None)\n",
    "        ax.set(ylabel=None)\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "    def get_decompositions_by_metric(self, metric: str, k: int, asc: bool) -> pd.DataFrame:\n",
    "        return self.all.sort_values(by=[metric], ascending=[asc]).head(k)\n",
    "\n",
    "    def get_sensitivity_analysis_results(self, metric: str, algorithm: str, show_plot: bool, title: str = None) -> Union[pd.DataFrame, plt.plot]:\n",
    "        if show_plot:\n",
    "            return self._show_sobol(self.sobol[algorithm][metric], algorithm, title)\n",
    "        return self.sobol[algorithm][metric]\n",
    "\n",
    "    def get_feature_importance(self, show_plot: bool, algorithm: str) -> Union[pd.DataFrame, plt.plot]:\n",
    "        if show_plot:\n",
    "            sns.heatmap(self.feature_scoring[algorithm], cmap='viridis', annot=True)\n",
    "            plt.title('Feature scoring')\n",
    "            return plt\n",
    "        return self.feature_scoring[algorithm]\n",
    "\n",
    "    def get_influential_parameters(self, metric: str, algorithm: str)-> pd.DataFrame:\n",
    "        if metric == \"all\":\n",
    "            max_index = self.feature_scoring[algorithm].values.argmax() // self.feature_scoring[algorithm].shape[1]\n",
    "            max_row = self.feature_scoring[algorithm].iloc[max_index]\n",
    "            max_row_df = pd.DataFrame(max_row).T  # Transpose to get it in the correct format\n",
    "            max_row_df.index = [self.feature_scoring[algorithm].index[max_index]]\n",
    "            return max_row_df\n",
    "        else:\n",
    "            return self.feature_scoring[algorithm].loc[[self.feature_scoring[algorithm][metric].idxmax()]]\n",
    "\n",
    "    def show_decomposition_structure(self, decomposition_index: str, decomposition: dict = None):\n",
    "        if not decomposition:\n",
    "            result = {key: val for subdict in self.partitions.values() for key, val in subdict.items()}\n",
    "            decomposition = result.get(decomposition_index)\n",
    "        decomposition = collections.OrderedDict(sorted(decomposition.items()))\n",
    "        return self._draw(decomposition)\n",
    "\n",
    "    def _get_class_color(self, class_name):\n",
    "        if self.class_color.get(class_name):\n",
    "            return self.class_color[class_name]\n",
    "        color = \"#\" + \"%06x\" % random.randint(0, 0xFFFFFF)\n",
    "        self.class_color[class_name] = color\n",
    "        return color\n",
    "\n",
    "    def _normalize_class_name(self, class_name):\n",
    "        if class_name.endswith('.java'):\n",
    "            return os.path.splitext(os.path.basename(class_name))[0]\n",
    "        return class_name.split('.')[-1]\n",
    "\n",
    "    def _draw(self, decomposition):\n",
    "        plt.rcParams['figure.figsize'] = [18, 18]\n",
    "        max_classes_len = -999999\n",
    "        ax = plt.subplot(211)\n",
    "        for microservice_id, microservice_classes in decomposition.items():\n",
    "            if len(microservice_classes) > max_classes_len:\n",
    "                max_classes_len = len(microservice_classes)\n",
    "            for position, class_name in enumerate(microservice_classes):\n",
    "                class_name = self._normalize_class_name(class_name)\n",
    "                color = self._get_class_color(class_name)\n",
    "                self.legends |= {class_name: mpatch.Patch(color=color, label=class_name)}\n",
    "                ax.add_artist(mpatch.Rectangle((position, microservice_id), 1, 1, color=color, fc = color))\n",
    "\n",
    "        labels = [f\"Microservice {id + 1}\" for id in decomposition.keys()]\n",
    "\n",
    "        ax.set_yticks(np.arange(len(labels)))\n",
    "        ax.set_yticklabels(labels)\n",
    "\n",
    "        ax.set_xlim((0, max_classes_len))\n",
    "        ax.set_ylim((0, len(decomposition.keys())))\n",
    "\n",
    "        for tick in ax.get_yticklabels():\n",
    "            tick.set_verticalalignment(\"bottom\")\n",
    "\n",
    "        ax.tick_params(size=0, axis='y', which='major', labelsize=20)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.grid(axis = 'y', color = 'black')\n",
    "        ax.legend(handles=self.legends.values(), bbox_to_anchor=(0., 1.07, 1., .102), loc=0,ncols=5, mode=\"expand\", borderaxespad=0., fontsize=10)\n",
    "        plt.tight_layout(pad=1.5)\n",
    "\n",
    "        return ax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SemanticLayer:\n",
    "    SYSTEM_PROMPT = \"\"\"You are an expert software architect that assists users to explore and understand a decomposition space.\n",
    "        You have a deep understanding of monolith to microservices migration and microservices quality metrics.\n",
    "        Your role is to help users to understand the decomposition space to pick the most suitable microservices decomposition according to the user need.\n",
    "        \"\"\"\n",
    "\n",
    "    NEW_PANDAS_RESPONSE_PROMPT = PromptTemplate(\"\"\"\n",
    "        Your are an expert software designer and also a translator from Pandas to English.\n",
    "        Given an input query and a Pandas tabular output, create a textual response based on the output.\n",
    "        Both the input query and the output are related to the exploration of microservices decomposition alternatives.\n",
    "        In your response, include all relevant details. Always include the relevant indexes of the Pandas dataframe you used to answer.\n",
    "        If the answer includes several results, describe them all and always pick the first one to exemplify and give a more detailed answer along its index and decomposition id.\n",
    "        When possible, use a bullet list to enumerate key points and include that data in tabular format including all metadata available (including indexes) to look for records.\n",
    "        Do not mention that the response is based on a Pandas structure or output. Instead, focus on the content of the output.\n",
    "        Do not make up information that is not provided in the Pandas dataframe. If you do not know the answer, respond \"I don't know\".\n",
    "\n",
    "        Query: {query_str}\n",
    "\n",
    "        Pandas Output: {pandas_output}\n",
    "\n",
    "        Response: \"\"\"\n",
    "    )\n",
    "\n",
    "    def __init__(self, decomposition_space=None, evaluation_mode=False) -> None:\n",
    "        self.llm = Groq(model=\"llama3-70b-8192\", api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n",
    "        self.tool_llm = Groq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n",
    "        self.intent_detector = None\n",
    "        self.decomposition_space = decomposition_space\n",
    "        self.evaluation_mode = evaluation_mode\n",
    "        tools = self._get_tools()\n",
    "        self.agent = OpenAIAgent.from_tools(tools, llm=self.tool_llm, system_prompt=SemanticLayer.SYSTEM_PROMPT, verbose=self.evaluation_mode)\n",
    "        encoder = HuggingFaceEncoder()\n",
    "        self.intent_detector = RouteLayer(encoder=encoder, routes=SemanticLayer._configure_routes(), llm=self.tool_llm)\n",
    "        self.client = GroqV2(api_key=GROQ_API_KEY)\n",
    "        if self.evaluation_mode:\n",
    "            Settings.llm = self.llm\n",
    "            sobol_qe = PandasQueryEngine(df=decomposition_space.sobol_df, synthesize_response=True, verbose=self.evaluation_mode)\n",
    "            sobol_qe.update_prompts({\"response_synthesis_prompt\": self.NEW_PANDAS_RESPONSE_PROMPT})\n",
    "            sobol_tool = QueryEngineTool.from_defaults(\n",
    "                query_engine=sobol_qe,\n",
    "                description=\"Useful for retrieving data about Sobol sensitivity analysis of each parameter.\",\n",
    "            )\n",
    "\n",
    "            all_qe = PandasQueryEngine(df=decomposition_space.all, synthesize_response=True, verbose=self.evaluation_mode)\n",
    "            all_qe.update_prompts({\"response_synthesis_prompt\": self.NEW_PANDAS_RESPONSE_PROMPT})\n",
    "            all_tool = QueryEngineTool.from_defaults(\n",
    "                query_engine=all_qe,\n",
    "                description=\"Useful for retrieving parameters and metrics (non-extreme distribution or, density, modularity, and number of partitions/microservices) of each decomposition.\",\n",
    "            )\n",
    "\n",
    "            partition_qe = PandasQueryEngine(df=decomposition_space.partitions_df, synthesize_response=True, verbose=self.evaluation_mode)\n",
    "            partition_qe.update_prompts({\"response_synthesis_prompt\": self.NEW_PANDAS_RESPONSE_PROMPT})\n",
    "            partition_tool = QueryEngineTool.from_defaults(\n",
    "                query_engine=partition_qe,\n",
    "                description=\"Useful for retrieving classes and the number of microservices/partitions of each decomposition. It contains the classes that are included in a microservice of a decomposition.\",\n",
    "            )\n",
    "\n",
    "            self.query_engine = RouterQueryEngine(\n",
    "                selector=LLMSingleSelector.from_defaults(),\n",
    "                query_engine_tools=[\n",
    "                    sobol_tool,\n",
    "                    all_tool,\n",
    "                    partition_tool\n",
    "                ],\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def get_decomposition_space(self, algorithm: str) -> plt.plot:\n",
    "        \"\"\" Get the decomposition space graphically.\n",
    "\n",
    "        Args:\n",
    "           algorithm (str): The algorithm or tool to get decomposition space from\n",
    "\n",
    "        Returns:\n",
    "            plt.plot: The plot of the decomposition space, showing a 2D projection of each decomposition as well as identification of the cluster each decomposition is into\n",
    "        \"\"\"\n",
    "        return self.decomposition_space.get_decomposition_space(algorithm)\n",
    "\n",
    "    def get_stable_solutions(self) -> plt.plot:\n",
    "        \"\"\" Get the stable decompositions/solutions graphically.\n",
    "\n",
    "        Returns:\n",
    "            plt.plot: The plot of the stable decompositions/solutions space, showing a 2D projection of each decomposition as well as identification of the cluster each decomposition is into\n",
    "        \"\"\"\n",
    "        return self.decomposition_space.get_stable_solutions()\n",
    "\n",
    "\n",
    "    def get_decomposition_by_metric(self, k: int, metric: str, asc: bool, show_plot: bool) -> Union[pd.DataFrame, plt.plot]:\n",
    "        \"\"\" Get the K decompositions that match a preferred metric (ned, density, modularity, stability, or n_partitions)).\n",
    "\n",
    "          Args:\n",
    "            k (int): The number of decompositions to retrieve.\n",
    "            metric (str): The metric to match the decompositions against, one of ned, density, modularity, stability, or number of partitions (n_partitions).\n",
    "            asc (bool): Whether to prefer lower values (True) or higher values (False).\n",
    "            show_plot (bool): Whether to return the decompositions that match a metric graphically through a plot\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The K decompositions are ordered by asc param against the metric passed as a parameter.\n",
    "            plt.plot: The plot of the K decompositions in the 2D projection space\n",
    "        \"\"\"\n",
    "        decompositions = self.decomposition_space.get_decompositions_by_metric(metric, k, asc)\n",
    "        indexes = decompositions.index.values\n",
    "        return self.decomposition_space.get_decomposition_space(decompositions.index.values)\n",
    "\n",
    "    def get_influential_parameters(self, algorithm: str, metric: str) -> pd.DataFrame:\n",
    "        \"\"\" Get the most influential parameters of a decomposition tool for a preferred metric (ned, density, modularity, n_partitions, all).\n",
    "\n",
    "         Args:\n",
    "           algorithm (str): The algorithm or tool to get parameters\n",
    "           metric (str):  The metric to match the decompositions against, one of ned, density, modularity, number of partitions or microservices (n_partitions), or all.\n",
    "\n",
    "         Returns:\n",
    "            pd.DataFrame: The most influential parameters for a metric.\n",
    "        \"\"\"\n",
    "        return self.decomposition_space.get_influential_parameters(metric, algorithm)\n",
    "\n",
    "    def show_decomposition_structure(self, decomposition_id: str) -> plt.plot:\n",
    "        \"\"\" Get the microservices of a decomposition obtained by a decomposition id.\n",
    "\n",
    "        Args:\n",
    "          decomposition_id (str): The decomposition id to obtain the structure from.\n",
    "\n",
    "        Returns:\n",
    "          plt.plot: The microservices architecture, each row represents a microservice, and each column represents a class inside that includes number of microservices (rows) and classes (columns) that compose each microservice distinguished by different colors\n",
    "        \"\"\"\n",
    "        return self.decomposition_space.show_decomposition_structure(decomposition_id)\n",
    "\n",
    "    def _get_tools(self):\n",
    "        return [\n",
    "            FunctionTool.from_defaults(fn=self.get_decomposition_space, return_direct=True),\n",
    "            FunctionTool.from_defaults(fn=self.get_decomposition_by_metric, return_direct=True),\n",
    "            FunctionTool.from_defaults(fn=self.get_influential_parameters, return_direct=True),\n",
    "            FunctionTool.from_defaults(fn=self.show_decomposition_structure, return_direct=True),\n",
    "            FunctionTool.from_defaults(fn=self.get_stable_solutions, return_direct=True)\n",
    "        ]\n",
    "\n",
    "    def _configure_routes():\n",
    "        return [\n",
    "            Route(\n",
    "                name=\"get_decomposition_space\",\n",
    "                utterances=[\n",
    "                    \"Which decompositions are generated?\",\n",
    "                    \"Show me the decomposition space graphically\",\n",
    "                    \"Get all decompositions graphically\",\n",
    "                    \"Show me all decompositions\",\n",
    "                    \"Show the decomposition space\",\n",
    "                    \"Show the decomposition space graphically\",\n",
    "                    \"Show me a visual representation of the decomposition space, including the 2D projection of each decomposition and its cluster.\",\n",
    "                    \"Provide the decomposition space plot to help visualize how decompositions are grouped in the 2D space.\",\n",
    "                    \"Give me a plot showing the decomposition space with each decomposition's 2D projection and its cluster.\"\n",
    "                ],\n",
    "                description=\"Get the decomposition space graphically.\"\n",
    "            ),\n",
    "            Route(\n",
    "                name=\"get_decomposition_by_metric\",\n",
    "                utterances=[\n",
    "                    \"Get the X decompositions with less Y\",\n",
    "                    \"Get me X decompositions with more Y\",\n",
    "                    \"Which is the decomposition with more X?\",\n",
    "                    \"Which is the decomposition with less X?\",\n",
    "                    \"Which is the decomposition of highest X?\",\n",
    "                    \"Which is the decomposition of lowest X?\",\n",
    "                    \"Show me the decomposition with more X\",\n",
    "                    \"Get a plot of the decomposition X\",\n",
    "                    \"Show me the decomposition with more X graphically\",\n",
    "                    \"Show me the decomposition with lowest X graphically\",\n",
    "                    \"Show me the decomposition with highest X graphically\",\n",
    "                    \"Show me the decomposition of lowest X graphically\",\n",
    "                    \"Show me the decomposition of highest X graphically\",\n",
    "                    \"Provide the K decompositions based on modularity with a plot showing the 2D projection of each.\",\n",
    "                    \"Show me a plot comparing decompositions sorted by stability, with higher values preferred.\",\n",
    "                    \"Display a plot of the K decompositions ordered by density.\"\n",
    "                    \"Show me the K decompositions ordered by the cohesion or coupling metric, with higher values preferred.\",\n",
    "                    \"Give me the decomposition sorted by modularity, showing the top K results with either ascending or descending values.\",\n",
    "                    \"I need to see the decompositions ranked by density, with a plot showing the comparison.\",\n",
    "                    \"Provide the top K decompositions based on the ned metric, ordered by highest to lowest value.\"\n",
    "                ],\n",
    "                description=\"Get the K decompositions that match a preferred metric, graphically or raw.\"\n",
    "            ),\n",
    "            Route(\n",
    "                name=\"get_influential_parameters\",\n",
    "                utterances=[\n",
    "                    \"Which is the most influential parameter for X in algorithm Y?\"\n",
    "                    \"Which is the most influential parameter of algorithm Y?\",\n",
    "                     \"Give me the most influential parameters for the cohesion metric in the decomposition.\",\n",
    "                    \"What parameters influence the modularity metric the most in the decomposition of algorithm Y?\",\n",
    "                    \"Show me the influential parameters for the density metric across different microservices.\",\n",
    "                    \"List the parameters that have the greatest influence on the n_partitions (number of partitions) metric in the decomposition.\",\n",
    "                    \"Provide the influential parameters for the stability metric in the decomposition.\"\n",
    "                ],\n",
    "                description=\"Get the most influential parameter for a metric or all metrics in an algoirthm.\"\n",
    "            ),\n",
    "            Route(\n",
    "                name=\"show_decomposition_structure\",\n",
    "                utterances=[\n",
    "                    \"How is structured the decomposition obtained by setting X to Y and Z to W?\"\n",
    "                    \"Which are the classes of the microservices of the decomposition with parameters X=Y and Z=W?\",\n",
    "                    \"How is the architecture of the decomposition with parameters X=Y and Z=W?\",\n",
    "                    \"Show me the structure of the decomposition, including the microservices and the classes assigned to each one.\",\n",
    "                    \"Give me a breakdown of how many classes are assigned to each microservice in the decomposition.\",\n",
    "                    \"Provide the decomposition structure so I can see how the microservices are organized by classes.\",\n",
    "                    \"How are entities like X, Y, and Z distributed across microservices in the decomposition?\",\n",
    "                    \"Check if any microservices are specifically handling X in the decomposition.\"\n",
    "                ],\n",
    "                description=\"Get the decomposition structure.\"\n",
    "            ),\n",
    "            Route(\n",
    "                name=\"get_stable_solutions\",\n",
    "                utterances=[\n",
    "                    \"Plot the stable solutions in the decomposition space, with a 2D projection and cluster identification for each solution.\",\n",
    "                    \"Visualize the stable solutions in the decomposition space, showing their 2D projection and the clusters they belong to.\",\n",
    "                    \"Generate a graphical representation of the stable solutions in the decomposition space, highlighting clusters.\",\n",
    "                    \"Display a plot of the stable solutions within the decomposition space, with each solution's cluster identified.\",\n",
    "                    \"Show a 2D plot of the stable solutions in the decomposition space, emphasizing cluster identification.\",\n",
    "                    \"Provide a graphical visualization of stable solutions, showing their position in the decomposition space and the clusters they belong to.\",\n",
    "                    \"Return a plot highlighting the stable solutions within the decomposition space, along with their respective clusters.\",\n",
    "                    \"Generate a 2D projection of stable solutions in the decomposition space, identifying the clusters for each.\",\n",
    "                    \"Visualize the stable solutions in the decomposition space with a plot that highlights their clusters.\",\n",
    "                    \"Show a graphical plot of stable solutions, including a 2D projection and the clusters associated with each solution.\",\n",
    "                    \"Show the stable solutions\",\n",
    "                    \"Get the stable decompositions\"\n",
    "                ],\n",
    "                description=\"Get stable solutions/descompositions\"\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.agent.reset()\n",
    "\n",
    "    def chat(self, question):\n",
    "        graphical_response = None\n",
    "        textual_response = self.query_engine.query(question).response\n",
    "        intent = self.intent_detector(question + textual_response)\n",
    "        if intent.name is not None:\n",
    "            function_name = \"\\nTry to execute tool \"+intent.name if (intent.name is not None) and (intent.name != 'misc') else \"\"\n",
    "            graphical_response = self.agent.chat(question+textual_response+function_name)\n",
    "        return display(Markdown(f\"{textual_response}<br>{graphical_response.response if graphical_response else ''}\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decomposition_space = DecompositionSpace(PROJECT_PATH, PROJECT_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layer = SemanticLayer(decomposition_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer.chat(\"How many different kind of decompositions there are?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_TEXT_QA_PROMPT = \"\"\"\\\n",
    "You are an expert software developer that needs to decompose an objective-oriented monolithic application into microservices, while taking scalability and maintenance factors into consideration.\n",
    "For this monolithic application, you have several decomposition alternatives at your disposal, and you have to choose the decomposition that best fit your needs.\n",
    "Since the decompositions are generated by an automated tool, each decomposition includes information about the parameters used by the tool to obtain the decomposition,\n",
    "such as: the class partitioning and several microservices quality metrics.\n",
    "\n",
    "Monolithic application: {app_description}\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Your task is to generate {k} specific questions about the decompositions that should help you inform your selection of the best decomposition.\n",
    "\n",
    "The documentation is as follows:\n",
    "{query_str}\n",
    "\n",
    "Please ensure that the questions are based on the documentation provided above and are not compound, and explore different analysis perspectives with respect to the decompositions.\n",
    "The questions should be formatted as a JSON list.\n",
    "\"\"\"\n",
    "text_question_template = PromptTemplate(DEFAULT_TEXT_QA_PROMPT)\n",
    "text_question_template = PromptTemplate(text_question_template.format(k=50, app_description=\"Cargo Tracking. The main focus of Cargo Tracking is to move a Cargo (identified by a TrackingId) between two Locations through a RouteSpecification. Once a Cargo becomes available, it is associated with one of the Itineraries (lists of CarrierMovements), selected from existing Voyages. HandlingEvents then trace the progress of the Cargo on the Itinerary. The Delivery of a Cargo informs about its state, estimated arrival time, and being on track.\"))\n",
    "text_qa_template = text_question_template\n",
    "question_gen_query = (f\"Your task is to setup questions about the decompositions that should help you inform your selection of the best decomposition.\" )\n",
    "documentation = [Document(text=\"\"\"\n",
    "Factors:\n",
    "- Scalability: Microservices are independent. Therefore, their sizing is easily accomplished. For instance, if a microservice is receiving several requests and consequently needs computational resources, it can be scaled individually using a container manager. Scalability then measures the ability of a system to cope with a load increase without sacrificing performance.\n",
    "- Maintenance: Independence, reduced size, and limited context are characteristics that belong to microservices, which provide ease of maintenance. Therefore, it can be inferred that modification is straightforward, that is, it is possible to modify a microservice directly without major concerns about the impact that will be caused on the other microservices that make up the application, since that there is a low degree of coupling between them. In case of introducing errors to a microservice, they can be fixed more quickly.\n",
    " \"\"\")]\n",
    "data_generator = DatasetGenerator.from_documents(documentation, text_question_template=text_question_template, text_qa_template=text_qa_template, question_gen_query=question_gen_query)\n",
    "questions = data_generator.generate_questions_from_nodes()\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Manual filtering of relevant questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the number of microservices in each decomposition?\",\n",
    "    \"What is the average size of each microservice of each decomposition in terms of number of classes?\",\n",
    "    \"What is the largest microservice in each decomposition in terms of number of classes?\",\n",
    "    \"What is the smallest microservice in each decomposition in terms of number of classes?\",\n",
    "    \"What is the cohesion metric for each microservice in each decomposition?\",\n",
    "    \"What is the coupling metric between microservices in each decomposition?\",\n",
    "    \"What is the complexity metric for each microservice in the decomposition?\",\n",
    "    \"How many dependencies exist between microservices in each decomposition?\",\n",
    "    \"What is the average number of interfaces per microservice in each decomposition?\",\n",
    "    \"Which decomposition has the highest scalability potential?\",\n",
    "    \"Which decomposition has the lowest maintenance complexity?\",\n",
    "    \"How many Cargo-related classes are in each microservice of each decomposition?\",\n",
    "    \"How many Location-related classes are in each microservice of each decomposition?\",\n",
    "    \"How many RouteSpecification-related classes are in each microservice of each decomposition?\",\n",
    "    \"How many Itinerary-related classes are in each microservice of each decomposition?\",\n",
    "    \"How many CarrierMovement-related classes are in each microservice of each decomposition?\",\n",
    "    \"How many HandlingEvent-related classes are in each microservice of each decomposition?\",\n",
    "    \"How many Delivery-related classes are in each microservice of each decomposition?\",\n",
    "    \"Which decomposition has the most balanced distribution of classes across microservices?\",\n",
    "    \"Which decomposition has the least balanced distribution of classes across microservices?\",\n",
    "    \"Which decomposition has the highest degree of independence between microservices?\",\n",
    "    \"Which decomposition has the lowest degree of independence between microservices?\",\n",
    "    \"What is the average number of configuration files per microservice in each decomposition?\",\n",
    "    \"Which decomposition has the highest degree of reusability across microservices?\",\n",
    "    \"Which decomposition has the lowest degree of reusability across microservices?\",\n",
    "    \"Which decomposition has the highest degree of alignment with business capabilities?\",\n",
    "    \"Which decomposition has the lowest degree of alignment with business capabilities?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "queries = []\n",
    "responses = []\n",
    "\n",
    "layer = SemanticLayer(decomposition_space, True)\n",
    "\n",
    "for query in questions:\n",
    "    try:\n",
    "        response = layer.chat(query)\n",
    "        responses.append(response)\n",
    "    except:\n",
    "        responses.append(\"None\")\n",
    "    finally:\n",
    "        queries.append(query)\n",
    "        layer.clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    AnswerRelevancyEvaluator\n",
    ")\n",
    "import tqdm\n",
    "\n",
    "judges = {}\n",
    "judges[\"answer_relevancy\"] = AnswerRelevancyEvaluator()\n",
    "\n",
    "evals = {\n",
    "    \"answer_relevancy\": [],\n",
    "}\n",
    "\n",
    "for query, response in tqdm.tqdm(zip(queries, responses)):\n",
    "    answer_relevancy_result = judges[\"answer_relevancy\"].evaluate(\n",
    "        query=query,\n",
    "        response=response\n",
    "    )\n",
    "\n",
    "    evals[\"answer_relevancy\"].append(answer_relevancy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Some results do not exactly match the evaluation mode, so we correct the result based on some texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "extracted_scores = []\n",
    "filtered_results = [result for result in evals[\"answer_relevancy\"] if result.invalid_result]\n",
    "for result in filtered_results:\n",
    "    match = re.search(r\"(?:\\n\\n\\*\\*Final Result:\\*\\*\\s*|\\n\\nFinal Result:\\s*)\\[[^\\]]+\\]\\s*(\\d+(?:\\.\\d+)?)\", result.feedback)\n",
    "\n",
    "    if match:\n",
    "        score = match.group(1)\n",
    "        extracted_scores.append((result.query, score))\n",
    "\n",
    "final_results = []\n",
    "\n",
    "for result in evals[\"answer_relevancy\"]:\n",
    "    if result.invalid_result:\n",
    "        result.score = next((float(r[1])/2 for r in extracted_scores if r[0] == result.query), None)\n",
    "    final_results.append(result)\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_results[\"Answer relevancy score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_results.to_csv(f\"{PROJECT_PATH}/results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}